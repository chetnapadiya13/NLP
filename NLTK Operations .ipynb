{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNatural Language Processing is manipulation or understanding text or speech by any \\nsoftware or machine. An analogy is that humans interact, understand each other views, and respond with the appropriate answer. In NLP, this interaction, understanding, the response \\nis made by a computer instead of a human.\\n\\nApplication of NLP \\n Sentiment Analysis.\\n Chatbots & Virtual Assistants.\\n Text Classification.\\n Text Extraction.\\n Machine Translation.\\n Text Summarization.\\n Market Intelligence.\\n Auto-Correct.\\n \\n NLP is branch of AI \\n Tools used to make NLP based applications and Text Preprocessing are \\n NLTK\\n Spacy \\n etc.. \\n \\n we are gonna discuss NLP using NLTK\\n \\n What is NLTK?\\nNLTK stands for Natural Language Toolkit. This toolkit is one of the most\\npowerful NLP libraries which contains packages to make machines understand human\\nlanguage and reply to it with an appropriate response. \\nPreprocessing required for NLP is given here \\nTokenization,\\nStemming,\\nLemmatization,\\nPunctuation,\\nCharacter count,\\nword count etc...\\n\\nabove process helps computer to understand what we are trying to communicate with it.\\n\\n*****************************To install NLTK follow given commands *******************************\\nPython 3.7 and above version \\nanaconda will be added advantage \\nPip3 install nltk \\n\\nfor anaconda \\nconda install -c anaconda nltk\\n\\nonce NLTK available it have loads of other modules let's download it \\ngoto python console and \\n\\nimport nltk\\nnltk.download ()\\n\\nonce above command will hit it will open one window \\nNLTK Downloaded Window Opens. Click the Download Button to download the dataset.\\nThis process will take time, based on your internet connection\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Natural Language Processing is manipulation or understanding text or speech by any \n",
    "software or machine. An analogy is that humans interact, understand each other views, and respond with the appropriate answer. In NLP, this interaction, understanding, the response \n",
    "is made by a computer instead of a human.\n",
    "\n",
    "Application of NLP \n",
    " Sentiment Analysis.\n",
    " Chatbots & Virtual Assistants.\n",
    " Text Classification.\n",
    " Text Extraction.\n",
    " Machine Translation.\n",
    " Text Summarization.\n",
    " Market Intelligence.\n",
    " Auto-Correct.\n",
    " \n",
    " NLP is branch of AI \n",
    " Tools used to make NLP based applications and Text Preprocessing are \n",
    " NLTK\n",
    " Spacy \n",
    " etc.. \n",
    " \n",
    " we are gonna discuss NLP using NLTK\n",
    " \n",
    " What is NLTK?\n",
    "NLTK stands for Natural Language Toolkit. This toolkit is one of the most\n",
    "powerful NLP libraries which contains packages to make machines understand human\n",
    "language and reply to it with an appropriate response. \n",
    "Preprocessing required for NLP is given here \n",
    "Tokenization,\n",
    "Stemming,\n",
    "Lemmatization,\n",
    "Punctuation,\n",
    "Character count,\n",
    "word count etc...\n",
    "\n",
    "above process helps computer to understand what we are trying to communicate with it.\n",
    "\n",
    "*****************************To install NLTK follow given commands *******************************\n",
    "Python 3.7 and above version \n",
    "anaconda will be added advantage \n",
    "Pip3 install nltk \n",
    "\n",
    "for anaconda \n",
    "conda install -c anaconda nltk\n",
    "\n",
    "once NLTK available it have loads of other modules let's download it \n",
    "goto python console and \n",
    "\n",
    "import nltk\n",
    "nltk.download ()\n",
    "\n",
    "once above command will hit it will open one window \n",
    "NLTK Downloaded Window Opens. Click the Download Button to download the dataset.\n",
    "This process will take time, based on your internet connection\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test and installation \n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Hello', 'Itvedant', 'ITs', 'fun', 'to', 'learn', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "# Run NLTK script \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "filterdText=tokenizer.tokenize('Hello Hello Itvedant,ITs fun to learn NLP.')\n",
    "print(filterdText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn this program, the objective was to remove all type of punctuations from given text. We imported \"RegexpTokenizer\" which is a module of NLTK. It removes all the expression, symbol, character, numeric or any things whatever you want.\\nYou just have passed the regular Expression to the \"RegexpTokenizer\" module.\\nFurther, we tokenized the word using \"tokenize\" module. The output is stored in the \"filterdText\" variable.\\nAnd printed them using \"print().\"\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "In this program, the objective was to remove all type of punctuations from given text. We imported \"RegexpTokenizer\" which is a module of NLTK. It removes all the expression, symbol, character, numeric or any things whatever you want.\n",
    "You just have passed the regular Expression to the \"RegexpTokenizer\" module.\n",
    "Further, we tokenized the word using \"tokenize\" module. The output is stored in the \"filterdText\" variable.\n",
    "And printed them using \"print().\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTokenization \\n\\nNLP atural language processing is used for building applications such as\\nText classification, intelligent chatbot, sentimental analysis, language translation, etc. \\nIt becomes vital to understand the pattern in the text to achieve the above-stated purpose. \\nThese tokens are very useful for finding such patterns as well as is\\nconsidered as a base step for stemming and lemmatization.\\nNatural Language toolkit has very important module tokenize which further comprises of sub-modules\\n\\nword tokenize\\nsentence tokenize\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Tokenization \n",
    "\n",
    "NLP atural language processing is used for building applications such as\n",
    "Text classification, intelligent chatbot, sentimental analysis, language translation, etc. \n",
    "It becomes vital to understand the pattern in the text to achieve the above-stated purpose. \n",
    "These tokens are very useful for finding such patterns as well as is\n",
    "considered as a base step for stemming and lemmatization.\n",
    "Natural Language toolkit has very important module tokenize which further comprises of sub-modules\n",
    "\n",
    "word tokenize\n",
    "sentence tokenize\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God', 'is', 'Great', '!', 'I', 'won', 'a', 'lottery', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"God is Great! I won a lottery.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nword_tokenize module is imported from the NLTK library.\\nA variable \"text\" is initialized with two sentences.\\nText variable is passed in word_tokenize module and printed the result. \\nThis module breaks each word with punctuation which you can see in the output.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "word_tokenize module is imported from the NLTK library.\n",
    "A variable \"text\" is initialized with two sentences.\n",
    "Text variable is passed in word_tokenize module and printed the result. \n",
    "This module breaks each word with punctuation which you can see in the output.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentences Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God is Great!', 'I won a lottery.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"God is Great! I won a lottery.\"\n",
    "print(sent_tokenize(text))\n",
    "\n",
    "Output: ['God is Great!', 'I won a lottery ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn a line like the previous program, imported the sent_tokenize module.\\nWe have taken the same sentence. Further sent module parsed that sentences and show output.\\nIt is clear that this function breaks each sentence.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "In a line like the previous program, imported the sent_tokenize module.\n",
    "We have taken the same sentence. Further sent module parsed that sentences and show output.\n",
    "It is clear that this function breaks each sentence.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS (Part-Of-Speech) Tagging & Chunking with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPOS Tagging\\nParts of speech Tagging is responsible for reading the text in a language and assigning some specific token (Parts of Speech) to each word.\\n\\ne.g.\\n\\nInput: Everything to permit us.\\n\\nOutput: [('Everything', NN),('to', TO), ('permit', VB), ('us', PRP)]\\n\\nSteps Involved:\\n\\nTokenize text (word_tokenize)\\napply pos_tag to above step that is nltk.pos_tag(tokenize_text)\\nhttps://www.nltk.org/book/ch05.html get tags and detail idea\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "POS Tagging\n",
    "Parts of speech Tagging is responsible for reading the text in a language and assigning some specific token (Parts of Speech) to each word.\n",
    "\n",
    "e.g.\n",
    "\n",
    "Input: Everything to permit us.\n",
    "\n",
    "Output: [('Everything', NN),('to', TO), ('permit', VB), ('us', PRP)]\n",
    "\n",
    "Steps Involved:\n",
    "\n",
    "Tokenize text (word_tokenize)\n",
    "apply pos_tag to above step that is nltk.pos_tag(tokenize_text)\n",
    "https://www.nltk.org/book/ch05.html get tags and detail idea'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using Tagger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPOS tagger is used to assign grammatical information of each word of the sentence. \\nInstalling, Importing and downloading all the packages of NLTK is complete.\\nAbbreviation\\tMeaning\\nCC\\tcoordinating conjunction\\nCD\\tcardinal digit\\nDT\\tdeterminer\\nEX\\texistential there\\nFW\\tforeign word\\nIN\\tpreposition/subordinating conjunction\\nJJ\\tadjective (large)\\nJJR\\tadjective, comparative (larger)\\nJJS\\tadjective, superlative (largest)\\nLS\\tlist market\\nMD\\tmodal (could, will)\\nNN\\tnoun, singular (cat, tree)\\nNNS\\tnoun plural (desks)\\nNNP\\tproper noun, singular (sarah)\\nNNPS\\tproper noun, plural (indians or americans)\\nPDT\\tpredeterminer (all, both, half)\\nPOS\\tpossessive ending (parent\\\\ 's)\\nPRP\\tpersonal pronoun (hers, herself, him,himself)\\nPRP$\\tpossessive pronoun (her, his, mine, my, our )\\nRB\\tadverb (occasionally, swiftly)\\nRBR\\tadverb, comparative (greater)\\nRBS\\tadverb, superlative (biggest)\\nRP\\tparticle (about)\\nTO\\tinfinite marker (to)\\nUH\\tinterjection (goodbye)\\nVB\\tverb (ask)\\nVBG\\tverb gerund (judging)\\nVBD\\tverb past tense (pleaded)\\nVBN\\tverb past participle (reunified)\\nVBP\\tverb, present tense not 3rd person singular(wrap)\\nVBZ\\tverb, present tense with 3rd person singular (bases)\\nWDT\\twh-determiner (that, what)\\nWP\\twh- pronoun (who)\\nWRB\\twh- adverb (how)\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "POS tagger is used to assign grammatical information of each word of the sentence. \n",
    "Installing, Importing and downloading all the packages of NLTK is complete.\n",
    "Abbreviation\tMeaning\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective (large)\n",
    "JJR\tadjective, comparative (larger)\n",
    "JJS\tadjective, superlative (largest)\n",
    "LS\tlist market\n",
    "MD\tmodal (could, will)\n",
    "NN\tnoun, singular (cat, tree)\n",
    "NNS\tnoun plural (desks)\n",
    "NNP\tproper noun, singular (sarah)\n",
    "NNPS\tproper noun, plural (indians or americans)\n",
    "PDT\tpredeterminer (all, both, half)\n",
    "POS\tpossessive ending (parent\\ 's)\n",
    "PRP\tpersonal pronoun (hers, herself, him,himself)\n",
    "PRP$\tpossessive pronoun (her, his, mine, my, our )\n",
    "RB\tadverb (occasionally, swiftly)\n",
    "RBR\tadverb, comparative (greater)\n",
    "RBS\tadverb, superlative (biggest)\n",
    "RP\tparticle (about)\n",
    "TO\tinfinite marker (to)\n",
    "UH\tinterjection (goodbye)\n",
    "VB\tverb (ask)\n",
    "VBG\tverb gerund (judging)\n",
    "VBD\tverb past tense (pleaded)\n",
    "VBN\tverb past participle (reunified)\n",
    "VBP\tverb, present tense not 3rd person singular(wrap)\n",
    "VBZ\tverb, present tense with 3rd person singular (bases)\n",
    "WDT\twh-determiner (that, what)\n",
    "WP\twh- pronoun (who)\n",
    "WRB\twh- adverb (how)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nChunking\\nChunking is used to add more structure to the sentence by following parts of speech (POS) tagging.\\nIt is also known as shallow parsing. The resulted group of words is called \"chunks.\" \\nIn shallow parsing, there is maximum one level between roots and leaves while deep parsing comprises of more than one level.\\nShallow Parsing is also called light parsing or chunking.\\n\\nThe primary usage of chunking is to make a group of \"noun phrases.\" The parts of speech are combined with regular expressions.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Chunking\n",
    "Chunking is used to add more structure to the sentence by following parts of speech (POS) tagging.\n",
    "It is also known as shallow parsing. The resulted group of words is called \"chunks.\" \n",
    "In shallow parsing, there is maximum one level between roots and leaves while deep parsing comprises of more than one level.\n",
    "Shallow Parsing is also called light parsing or chunking.\n",
    "\n",
    "The primary usage of chunking is\n",
    "to make a group of \"noun phrases.\" \n",
    "The parts of speech are combined with \n",
    "regular expressions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRules for Chunking:\\n\\nThere are no pre-defined rules, but you can combine them according to need and requirement.\\n\\nFor example, you need to tag Noun, verb (past tense), adjective, and coordinating junction \\nfrom the sentence. You can use the rule as below\\n\\nchunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\\n\\nFollowing table shows what the various symbol means:\\n\\nName of symbol\\tDescription\\n. Any character except new line\\n* Match 0 or more repetitions\\n? Match 0 or 1 repetitions'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Rules for Chunking:\n",
    "\n",
    "There are no pre-defined rules, but you can combine\n",
    "them according to need and requirement.\n",
    "\n",
    "For example, you need to tag Noun, verb (past tense), adjective, and coordinating junction \n",
    "from the sentence. You can use the rule as below\n",
    "\n",
    "chunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\n",
    "\n",
    "Following table shows what the various symbol means:\n",
    "\n",
    "Name of symbol\tDescription\n",
    ". Any character except new line\n",
    "* Match 0 or more repetitions\n",
    "? Match 0 or 1 repetitions'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split: ['learn', 'Data', 'Science', 'from', 'itvedant', 'and', 'make', 'study', 'easy']\n",
      "After Token: [('learn', 'NN'), ('Data', 'NNP'), ('Science', 'NNP'), ('from', 'IN'), ('itvedant', 'NN'), ('and', 'CC'), ('make', 'VB'), ('study', 'NN'), ('easy', 'JJ')]\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "After Chunking (S\n",
      "  (mychunk learn/NN Data/NNP Science/NNP)\n",
      "  from/IN\n",
      "  (mychunk itvedant/NN and/CC)\n",
      "  make/VB\n",
      "  (mychunk study/NN easy/JJ))\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "text =\"learn Data Science from itvedant and make study easy\".split()\n",
    "print(\"After Split:\",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\",tokens_tag)\n",
    "patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunker)\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe conclusion from the above example: \"make\" is a verb which is not included in the rule, \\nso it is not tagged as mychunk\\n\\nUse Case of Chunking\\nChunking is used for entity detection. \\nAn entity is that part of the sentence by which machine get the value for any intention\\n\\nExample: \\nTemperature of New York. \\nHere Temperature is the intention and New York is an entity.\\n\\nIn other words, chunking is used as selecting the subsets of tokens.\\nPlease follow the below code to understand how chunking is used to select the tokens. In this example, \\nyou will see the graph which will correspond to a chunk of a noun phrase. \\nWe will write the code and draw the graph for better understanding.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The conclusion from the above example:\n",
    "\"make\" is a verb which is not included in the rule, \n",
    "so it is not tagged as mychunk\n",
    "\n",
    "Use Case of Chunking\n",
    "Chunking is used for entity detection. \n",
    "An entity is that part of the sentence by \n",
    "which machine get the value for any intention\n",
    "\n",
    "Example: \n",
    "Temperature of New York. \n",
    "Here Temperature is the intention and New York\n",
    "is an entity.\n",
    "\n",
    "In other words, chunking is used as selecting the subsets of tokens.\n",
    "Please follow the below code to understand how chunking is used to select the tokens. In this example, \n",
    "you will see the graph which will correspond to a chunk of a noun phrase. \n",
    "We will write the code and draw the graph for better understanding.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learn', 'Data', 'Science', 'from', 'itvedant']\n",
      "[('learn', 'NN'), ('Data', 'NNP'), ('Science', 'NNP'), ('from', 'IN'), ('itvedant', 'NN')]\n",
      "(S (NP learn/NN) Data/NNP Science/NNP from/IN (NP itvedant/NN))\n"
     ]
    }
   ],
   "source": [
    "text = \"learn Data Science from itvedant\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "tag = nltk.pos_tag(tokens)\n",
    "print(tag)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp  =nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tag)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.draw() # It will draw the pattern graphically which can be seen in Noun Phrase chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chunking is used to categorize different tokens into the same chunk. \\nThe result will depend on grammar which has been selected.\\nFurther chunking is used to tag patterns and to explore text corpora.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Chunking is used to categorize different tokens\n",
    "into the same chunk. \n",
    "The result will depend on grammar which has been \n",
    "selected.\n",
    "Further chunking is used to tag patterns and\n",
    "to explore text corpora.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stemming is a kind of normalization for words. Normalization is a technique where a set of words\\nin a sentence are converted into a sequence to shorten its lookup. The words which have the same\\nmeaning but have some variation according to \\nthe context or sentence are normalized.\\nIn another word, there is one root word, but there are many variations of the same words.\\nFor example, the root word is \"eat\" and it\\'s variations are \"eats, eating, eaten and like so\".\\nIn the same way, with the help of Stemming, we can find the root word of any variations.\\n\\nFor example \\nHe was riding.\\nHe was taking the ride.\\n\\nIn the above two sentences, the meaning is the same, i.e., riding activity in the past. \\nA human can easily understand that both meanings are the same. \\nBut for machines, both sentences are different. Thus it became hard to convert it into the same data row. In case we do not provide the same data-set, \\nthen machine fails to predict. So it is necessary to differentiate the meaning of each word to prepare the dataset for machine learning.\\nAnd here stemming is used to categorize the same type of data by getting its root word.\\n\\nNLTK has an algorithm named as \"PorterStemmer\".\\nThis algorithm accepts the list of tokenized word and stems it into root word.\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Stemming is a kind of normalization for words.\n",
    "Normalization is a technique where a set of words\n",
    "in a sentence are converted into a sequence to \n",
    "shorten its lookup. The words which have the same\n",
    "meaning but have some variation according to \n",
    "the context or sentence are normalized.\n",
    "In another word, there is one root word, \n",
    "but there are many variations of the same words.\n",
    "For example, the root word is \"eat\" and \n",
    "it's variations are \"eats, eating, eaten and \n",
    "like so\".\n",
    "In the same way, with the help of Stemming,\n",
    "we can find the root word of any variations.\n",
    "\n",
    "For example \n",
    "He was riding.\n",
    "He was taking the ride.\n",
    "\n",
    "In the above two sentences, the meaning is the same, i.e., riding activity in the past. \n",
    "A human can easily understand that both meanings are the same. \n",
    "But for machines, both sentences are different. Thus it became hard to convert it into the same data row. In case we do not provide the same data-set, \n",
    "then machine fails to predict. So it is necessary to differentiate the meaning of each word to prepare the dataset for machine learning.\n",
    "And here stemming is used to categorize the same type of data by getting its root word.\n",
    "\n",
    "NLTK has an algorithm named as \"PorterStemmer\".\n",
    "This algorithm accepts the list of tokenized word and stems it into root word.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "wait\n",
      "wait\n",
      "wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
    "ps =PorterStemmer()\n",
    "for w in e_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "itved\n",
      ",\n",
      "you\n",
      "have\n",
      "to\n",
      "build\n",
      "a\n",
      "veri\n",
      "good\n",
      "site\n",
      "and\n",
      "I\n",
      "love\n",
      "visit\n",
      "your\n",
      "site\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sentence=\"Hello itvedant, You have to build a very good site and I love visiting your site.\"\n",
    "words = word_tokenize(sentence)\n",
    "ps = PorterStemmer()\n",
    "for w in words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nLemmatization is the algorithmic process of finding the lemma of a word depending on their meaning.\\nLemmatization usually refers to the morphological analysis of words, which aims to remove inflectional endings.\\nIt helps in returning the base or dictionary form of a word, which is known as the lemma.\\nThe NLTK Lemmatization method is based on WorldNet's built-in morph function.\\nText preprocessing includes both stemming as well as lemmatization. Many people find the two terms confusing. \\nSome treat these as same, but there is a difference between these both.\\nLemmatization is preferred over the former because of the below reason.\\n\\nStemming algorithm works by cutting the suffix from the word. \\nIn a broader sense cuts either the beginning or end of the word.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Lemmatization is the algorithmic process of finding the lemma of a word depending on their meaning.\n",
    "Lemmatization usually refers to the morphological analysis of words, which aims to remove inflectional endings.\n",
    "It helps in returning the base or dictionary form of a word, which is known as the lemma.\n",
    "The NLTK Lemmatization method is based on WorldNet's built-in morph function.\n",
    "Text preprocessing includes both stemming as well as lemmatization. Many people find the two terms confusing. \n",
    "Some treat these as same, but there is a difference between these both.\n",
    "Lemmatization is preferred over the former because of the below reason.\n",
    "\n",
    "Stemming algorithm works by cutting the suffix from the word. \n",
    "In a broader sense cuts either the beginning or end of the word.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for studies is studi\n",
      "Stemming for studying is studi\n",
      "Stemming for cries is cri\n",
      "Stemming for cry is cri\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer  = PorterStemmer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Time example showing use of Wordnet Lemmatization and POS Tagging in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data => Data\n",
      "Science => Science\n",
      "is => be\n",
      "a => a\n",
      "totally => totally\n",
      "new => new\n",
      "kind => kind\n",
      "of => of\n",
      "learning => learn\n",
      "experience => experience\n",
      ". => .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "text = \"Data Science is a totally new kind of learning experience.\"\n",
    "tokens = word_tokenize(text)\n",
    "lemma_function = WordNetLemmatizer()\n",
    "for token, tag in pos_tag(tokens):\n",
    "    lemma = lemma_function.lemmatize(token, tag_map[tag[0]])\n",
    "    print(token, \"=>\", lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wordnet is an NLTK corpus reader, a lexical database for English. It can be used to find the meaning of words,\\nsynonym or antonym. One can define it as a semantically oriented dictionary of English.\\nIt is imported with the following command:'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Wordnet is an NLTK corpus reader, a lexical database for English. It can be used to find the meaning of words,\n",
    "synonym or antonym. One can define it as a semantically oriented dictionary of English.\n",
    "It is imported with the following command:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stats reveal that there are 155287 words and 117659 synonym sets included with English WordNet.\\n\\nDifferent methods available with WordNet can be found by typing dir'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''stats reveal that there are 155287 words and 117659 synonym sets included with English WordNet.\n",
    "\n",
    "Different methods available with WordNet can be found by typing dir'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADJ',\n",
       " 'ADJ_SAT',\n",
       " 'ADV',\n",
       " 'MORPHOLOGICAL_SUBSTITUTIONS',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " '_ENCODING',\n",
       " '_FILEMAP',\n",
       " '_FILES',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " '_compute_max_depth',\n",
       " '_data_file',\n",
       " '_data_file_map',\n",
       " '_encoding',\n",
       " '_exception_map',\n",
       " '_fileids',\n",
       " '_get_root',\n",
       " '_key_count_file',\n",
       " '_key_synset_file',\n",
       " '_lang_data',\n",
       " '_lemma_pos_offset_map',\n",
       " '_lexnames',\n",
       " '_load_exception_map',\n",
       " '_load_lang_data',\n",
       " '_load_lemma_pos_offset_map',\n",
       " '_max_depth',\n",
       " '_morphy',\n",
       " '_omw_reader',\n",
       " '_pos_names',\n",
       " '_pos_numbers',\n",
       " '_root',\n",
       " '_synset_from_pos_and_line',\n",
       " '_synset_from_pos_and_offset',\n",
       " '_synset_offset_cache',\n",
       " '_tagset',\n",
       " '_unload',\n",
       " 'abspath',\n",
       " 'abspaths',\n",
       " 'all_lemma_names',\n",
       " 'all_synsets',\n",
       " 'citation',\n",
       " 'custom_lemmas',\n",
       " 'encoding',\n",
       " 'ensure_loaded',\n",
       " 'fileids',\n",
       " 'get_version',\n",
       " 'ic',\n",
       " 'jcn_similarity',\n",
       " 'langs',\n",
       " 'lch_similarity',\n",
       " 'lemma',\n",
       " 'lemma_count',\n",
       " 'lemma_from_key',\n",
       " 'lemmas',\n",
       " 'license',\n",
       " 'lin_similarity',\n",
       " 'morphy',\n",
       " 'of2ss',\n",
       " 'open',\n",
       " 'path_similarity',\n",
       " 'readme',\n",
       " 'res_similarity',\n",
       " 'root',\n",
       " 'ss2of',\n",
       " 'synset',\n",
       " 'synset_from_pos_and_offset',\n",
       " 'synset_from_sense_key',\n",
       " 'synsets',\n",
       " 'unicode_repr',\n",
       " 'words',\n",
       " 'wup_similarity']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLet us understand some of the features available with the wordnet:\\n\\nSynset: It is also called as synonym set or collection of synonym words. Let us check a example\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Let us understand some of the features available with the wordnet:\n",
    "\n",
    "Synset: It is also called as synonym set or collection of synonym words. Let us check a example\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets(\"dog\")\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLexical Relations: These are semantic relations which are reciprocated.\\nIf there is a relationship between {x1,x2,...xn} and {y1,y2,...yn} then there is also relation between {y1,y2,...yn} and {x1,x2,...xn}. \\nFor example Synonym is the opposite of antonym or hypernyms and hyponym are type of lexical concept.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Lexical Relations: These are semantic relations which are reciprocated.\n",
    "If there is a relationship between {x1,x2,...xn} and {y1,y2,...yn} then there is also relation between {y1,y2,...yn} and {x1,x2,...xn}. \n",
    "For example Synonym is the opposite of antonym or hypernyms and hyponym are type of lexical concept.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'active', 'active_agent'}\n",
      "set()\n",
      "{'active_voice', 'active', 'active_agent'}\n",
      "{'passive_voice'}\n",
      "{'active_voice', 'active', 'active_agent'}\n",
      "{'passive_voice'}\n",
      "{'active_voice', 'active', 'active_agent'}\n",
      "{'passive_voice', 'inactive'}\n",
      "{'active_voice', 'fighting', 'combat-ready', 'active', 'active_agent'}\n",
      "{'passive_voice', 'inactive'}\n",
      "{'active_voice', 'fighting', 'combat-ready', 'active', 'active_agent'}\n",
      "{'passive_voice', 'inactive', 'passive'}\n",
      "{'active_voice', 'fighting', 'combat-ready', 'active', 'participating', 'active_agent'}\n",
      "{'passive_voice', 'inactive', 'passive'}\n",
      "{'active_voice', 'fighting', 'combat-ready', 'active', 'participating', 'active_agent'}\n",
      "{'passive_voice', 'inactive', 'passive'}\n",
      "{'active_voice', 'fighting', 'combat-ready', 'active', 'participating', 'active_agent'}\n",
      "{'passive_voice', 'inactive', 'passive'}\n",
      "{'active_voice', 'fighting', 'combat-ready', 'active', 'participating', 'active_agent'}\n",
      "{'passive_voice', 'inactive', 'passive'}\n",
      "{'active_voice', 'fighting', 'combat-ready', 'alive', 'active', 'participating', 'active_agent'}\n",
      "{'passive_voice', 'inactive', 'passive'}\n",
      "{'active_voice', 'fighting', 'combat-ready', 'alive', 'active', 'participating', 'active_agent'}\n",
      "{'quiet', 'passive_voice', 'inactive', 'passive'}\n",
      "{'active_voice', 'fighting', 'combat-ready', 'alive', 'active', 'participating', 'active_agent'}\n",
      "{'quiet', 'passive_voice', 'inactive', 'passive'}\n",
      "{'active_voice', 'fighting', 'combat-ready', 'dynamic', 'alive', 'active', 'participating', 'active_agent'}\n",
      "{'passive_voice', 'stative', 'inactive', 'quiet', 'passive'}\n",
      "{'active_voice', 'fighting', 'combat-ready', 'dynamic', 'alive', 'active', 'participating', 'active_agent'}\n",
      "{'extinct', 'passive_voice', 'stative', 'inactive', 'quiet', 'passive'}\n",
      "{'active_voice', 'fighting', 'combat-ready', 'dynamic', 'alive', 'active', 'participating', 'active_agent'}\n",
      "{'extinct', 'passive_voice', 'stative', 'inactive', 'quiet', 'dormant', 'passive'}\n",
      "{'active_voice', 'fighting', 'combat-ready', 'dynamic', 'alive', 'active', 'participating', 'active_agent'}\n",
      "{'extinct', 'passive_voice', 'stative', 'inactive', 'quiet', 'dormant', 'passive'}\n"
     ]
    }
   ],
   "source": [
    "#Let us write a program using python to find synonym and antonym of word \"active\" using Wordnet.\n",
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"active\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "    print(set(synonyms))\n",
    "    print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Wordnet is a corpus, so it is imported from the ntlk.corpus\n",
    "List of both synonym and antonym is taken as empty which will be used for appending\n",
    "Synonyms of the word active are searched in the module synsets and are appended in the list synonyms.\n",
    "The same process is repeated for the second one.\n",
    "Output is printed '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tagging Sentence in a broader sense refers to the addition of labels of the verb,\n",
    "noun,etc.by the context of the sentence. Identification of POS tags is a complicated process.\n",
    "Thus generic tagging of POS is manually not possible as some words may have different (ambiguous) meanings according to \n",
    "the structure of the sentence. Conversion of text in the form of list is an important step before tagging as each word in \n",
    "the list is looped and counted for a particular tag. \n",
    "Please see the below code to understand it better'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('Itvedant', 'NNP'), (',', ','), ('You', 'PRP'), ('have', 'VBP'), ('to', 'TO'), ('build', 'VB'), ('a', 'DT'), ('very', 'RB'), ('good', 'JJ'), ('site', 'NN'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('love', 'VBP'), ('visiting', 'VBG'), ('your', 'PRP$'), ('site', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello Itvedant, You have to build a very good site, and I love visiting your   site.\"\n",
    "sentence = nltk.sent_tokenize(text)\n",
    "for sent in sentence:\n",
    "\t print(nltk.pos_tag(nltk.word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code Explanation\n",
    "\n",
    "Code to import nltk (Natural language toolkit which contains submodules such as sentence tokenize and word tokenize.)\n",
    "Text whose tags are to be printed.\n",
    "Sentence Tokenization\n",
    "For loop is implemented where words are tokenized from sentence and tag of each word is printed as output.\n",
    "In Corpus there are two types of POS taggers:\n",
    "\n",
    "Rule-Based :rules of grammer applied \n",
    "Stochastic POS Taggers : FREQUANCY AND PROBABILITY APPLIED \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting POS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "you will study how to count these tags. \n",
    "Counting tags are crucial for text classification \n",
    "as well as \n",
    "preparing the features for the Natural \n",
    "language-based operations\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts =>  Counter({'NN': 5, 'NNS': 2, ',': 2, 'VBZ': 1, 'CD': 1, 'IN': 1, 'DT': 1, 'JJS': 1, 'TO': 1, 'VB': 1, 'CC': 1, 'RB': 1, 'JJR': 1, '.': 1})\n",
      "Tags =>  [('itvedant', 'NN'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('sites', 'NNS'), ('to', 'TO'), ('learn', 'VB'), ('data', 'NNS'), ('science', 'NN'), (',', ','), ('php', 'NN'), (',', ','), ('java', 'NN'), ('and', 'CC'), ('much', 'RB'), ('more', 'JJR'), ('online', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "text = \" Itvedant is one of the best sites to learn Data Science,PHP,Java and much more online.\"\n",
    "lower_case = text.lower()\n",
    "tokens = nltk.word_tokenize(lower_case)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "counts = Counter( tag for word,  tag in tags)\n",
    "print(\"counts => \",counts)\n",
    "print(\"Tags => \",tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To count the tags, you can use the package Counter from the collection's module. \n",
    "A counter is a dictionary subclass which works on the principle of key-value operation.\n",
    "It is an unordered collection where elements are stored as a dictionary key while the count is their value.\n",
    "Import nltk which contains modules to tokenize the text.\n",
    "Write the text whose pos_tag you want to count.\n",
    "Some words are in upper case and some in lower case, so it is appropriate to transform all the words in\n",
    "the lower case before applying tokenization.\n",
    "Pass the words through word_tokenize from nltk.\n",
    "Calculate the pos_tag of each token'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Frequency Distribution is referred to as the number of times an outcome of an experiment occurs. \n",
    "It is used to find the frequency of each word occurring in a document. It uses FreqDistclass and defined by the nltk.\n",
    "probabilty module.\n",
    "\n",
    "A frequency distribution is usually created by counting the samples of repeatedly running the experiment.\n",
    "The no of counts is incremented by one, each time. E.g.\n",
    "\n",
    "freq_dist = FreqDist()\n",
    "\n",
    "for the token in the document:\n",
    "\n",
    "freq_dist.inc(token.type())\n",
    "\n",
    "For any word, we can check how many times it occurred in a particular document. E.g.\n",
    "\n",
    "Count Method: freq_dist.count('and')This expression returns the value of the number of times 'and' occurred. \n",
    "    It is called the count method.\n",
    "    \n",
    "Frequency Method: freq_dist.freq('and')This the expression returns frequency of a given sample.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2502f2e4ac8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"Itvedant is the site where you can find the best tutorials for Software development Tutorial, Data Science Course for Beginners. Java Tutorial for Beginners and much more. Please visit the site www.itvedant.com and much more.\"\n",
    "words = nltk.tokenize.word_tokenize(a)\n",
    "fd = nltk.FreqDist(words)\n",
    "fd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Explanation of code:\n",
    "\n",
    "Import nltk module.\n",
    "Write the text whose word distribution you need to find.\n",
    "Tokenize each word in the text which is served as input to FreqDist module of the nltk.\n",
    "Apply each word to nlk.FreqDist in the form of a list\n",
    "Plot the words in the graph using plot() '''\n",
    "\n",
    "'''\n",
    "Observe the graph above. It corresponds to counting the occurrence of each word in the text.\n",
    "It helps in the study of text and further in implementing text-based sentimental analysis. \n",
    "In a nutshell, it can be concluded that nltk has a module for counting the occurrence of \n",
    "each word in the text which helps in preparing the stats of natural language features.\n",
    "It plays a significant role in finding the keywords in the text. '''\n",
    "\n",
    "Note :- \n",
    "    Counting each word may not be much useful. Instead one should focus on collocation \n",
    "    and bigrams which deals with a lot of words in a pair. These pairs identify useful\n",
    "    keywords to better natural language \n",
    "    features which can be fed to the machine. Please look below for their details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocations: Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "What is Collocations?\n",
    "Collocations are the pairs of words occurring together many times in a document. \n",
    "It is calculated by the number of those pair occurring together to the overall word count of\n",
    "the document.\n",
    "\n",
    "Example :- \n",
    "Consider electromagnetic spectrum with words like ultraviolet rays, infrared rays.\n",
    "The words ultraviolet and rays are not used individually and hence can be treated as \n",
    "Collocation. Another example is the CT Scan.\n",
    "We don't say CT and Scan separately, and hence they are also treated as collocation.\n",
    "\n",
    "********************************************************\n",
    "\n",
    "We can say that finding collocations requires calculating the frequencies of words and their appearance \n",
    "in the context of other\n",
    "words. These specific collections of words require filtering to retain useful content terms. \n",
    "Each gram of words may then be scored according to some association measure, to determine the relative likelihood of each Ingram being a collocation.\n",
    "\n",
    "Collocation can be categorized into 3 types types-\n",
    "\n",
    "Bigrams combination of two words\n",
    "Trigrams combination of three words\n",
    "ngrams combination of more than 3 words\n",
    "\n",
    "Bigrams and Trigrams provide more meaningful and useful features for the feature extraction \n",
    "stage. \n",
    "\n",
    "These are especially useful in text-based sentimental analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams Combination of two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Itvedant', 'is'), ('is', 'a'), ('a', 'totally'), ('totally', 'new'), ('new', 'kind'), ('kind', 'of'), ('of', 'learning'), ('learning', 'experience'), ('experience', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = \"Itvedant is a totally new kind of learning experience.\"\n",
    "Tokens = nltk.word_tokenize(text)\n",
    "output = list(nltk.bigrams(Tokens))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigrams Combination of three words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Itvedant', 'is', 'a'), ('is', 'a', 'totally'), ('a', 'totally', 'new'), ('totally', 'new', 'kind'), ('new', 'kind', 'of'), ('kind', 'of', 'learning'), ('of', 'learning', 'experience'), ('learning', 'experience', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = \"Itvedant is a totally new kind of learning experience.\"\n",
    "Tokens = nltk.word_tokenize(text)\n",
    "output = list(nltk.trigrams(Tokens))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>good movie</th>\n",
       "      <th>like</th>\n",
       "      <th>movie</th>\n",
       "      <th>not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   good movie      like     movie       not\n",
       "0    0.707107  0.000000  0.707107  0.000000\n",
       "1    0.577350  0.000000  0.577350  0.577350\n",
       "2    0.000000  0.707107  0.000000  0.707107\n",
       "3    0.000000  1.000000  0.000000  0.000000\n",
       "4    0.000000  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "texts = [\"good movie\",\"not a good movie\", \"did not like\",\n",
    "        \"i like it\",\"good one\"]\n",
    "tfidf = TfidfVectorizer(min_df=2,max_df=0.5,ngram_range=(1,2))\n",
    "\n",
    "features = tfidf.fit_transform(texts)\n",
    "pd.DataFrame(features.todense(),columns=tfidf.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
